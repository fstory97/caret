# VS Code AI ì½”ë”© ì—ì´ì „íŠ¸ í™•ì¥ ê¸°ëŠ¥ - ë‹¤ì¤‘ Ollama ì§€ì›

## 1. ê°œìš”

OllamaëŠ” ë¡œì»¬ í™˜ê²½ì—ì„œ ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ AI ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. VS Code AI ì½”ë”© ì—ì´ì „íŠ¸ì— ë‹¤ì¤‘ Ollama ì§€ì›ì„ ì¶”ê°€í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì´ ìˆìŠµë‹ˆë‹¤:

- 100% ë¡œì»¬ ê°œë°œ í™˜ê²½ êµ¬ì¶• ê°€ëŠ¥
- ë¯¼ê°í•œ ì½”ë“œ ë°ì´í„°ë¥¼ ì™¸ë¶€ ì„œë²„ë¡œ ì „ì†¡í•˜ì§€ ì•ŠìŒ
- ë‹¤ì–‘í•œ AI ëª¨ë¸ì„ ìƒí™©ì— ë§ê²Œ ì „í™˜í•˜ì—¬ ì‚¬ìš©
- ì¸í„°ë„· ì—°ê²° ì—†ì´ë„ AI ì½”ë”© ì§€ì› ê°€ëŠ¥

```mermaid
graph TD
    A[VS Code í™•ì¥] --> B[AI ëª¨ë¸ ì»¤ë„¥í„°]
    B --> C[OpenAI ì»¤ë„¥í„°]
    B --> D[Claude ì»¤ë„¥í„°]
    B --> E[Ollama ì»¤ë„¥í„°]
    E --> F[Llama 3]
    E --> G[CodeLlama]
    E --> H[Mistral]
    E --> I[ê¸°íƒ€ ë¡œì»¬ ëª¨ë¸]
```

## 2. êµ¬í˜„ ë°©ë²•

### 2.1 Ollama ëª¨ë¸ ì»¤ë„¥í„° êµ¬í˜„

```typescript
// ai/models/ollamaModel.ts
import { AIModel, CompletionOptions } from './baseModel';
import axios from 'axios';

export interface OllamaModelConfig {
    endpoint: string;
    modelName: string;
    contextSize: number;
}

export class OllamaModel implements AIModel {
    private endpoint: string;
    private modelName: string;
    private maxContextTokens: number;

    constructor(config: OllamaModelConfig) {
        this.endpoint = config.endpoint || 'http://localhost:11434';
        this.modelName = config.modelName;
        this.maxContextTokens = config.contextSize || 8192;
    }

    async generateCompletion(prompt: string, options?: CompletionOptions): Promise<string> {
        try {
            const response = await axios.post(`${this.endpoint}/api/generate`, {
                model: this.modelName,
                prompt: prompt,
                options: {
                    temperature: options?.temperature || 0.7,
                    top_p: options?.topP || 0.9,
                    max_tokens: options?.maxTokens || 1000,
                    stop: options?.stop || []
                },
                stream: false
            });
            
            return response.data.response;
        } catch (error) {
            console.error('Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜:', error);
            throw new Error('ë¡œì»¬ AI ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
        }
    }

    async generateCompletionStream(prompt: string, options?: CompletionOptions): Promise<ReadableStream<string>> {
        try {
            const response = await axios.post(
                `${this.endpoint}/api/generate`,
                {
                    model: this.modelName,
                    prompt: prompt,
                    options: {
                        temperature: options?.temperature || 0.7,
                        top_p: options?.topP || 0.9,
                        max_tokens: options?.maxTokens || 1000,
                        stop: options?.stop || []
                    },
                    stream: true
                },
                { responseType: 'stream' }
            );
            
            return response.data;
        } catch (error) {
            console.error('Ollama ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜:', error);
            throw new Error('ë¡œì»¬ AI ëª¨ë¸ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
        }
    }

    estimateTokens(text: string): number {
        // ê°„ë‹¨í•œ í† í° ì¶”ì • - ì‹¤ì œë¡œëŠ” ëª¨ë¸ë³„ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
        return Math.ceil(text.length / 3.5);
    }

    getMaxContextTokens(): number {
        return this.maxContextTokens;
    }
}
```

### 2.2 Ollama ëª¨ë¸ ê´€ë¦¬ì êµ¬í˜„

ì—¬ëŸ¬ Ollama ëª¨ë¸ì„ ê´€ë¦¬í•˜ê³  ì „í™˜í•  ìˆ˜ ìˆëŠ” ê´€ë¦¬ì í´ë˜ìŠ¤:

```typescript
// ai/models/ollamaManager.ts
import * as vscode from 'vscode';
import { OllamaModel, OllamaModelConfig } from './ollamaModel';
import axios from 'axios';

export class OllamaManager {
    private models: Map<string, OllamaModel> = new Map();
    private activeModelName: string | null = null;
    private endpoint: string;
    
    constructor(endpoint: string = 'http://localhost:11434') {
        this.endpoint = endpoint;
    }
    
    async initialize(): Promise<void> {
        try {
            await this.refreshAvailableModels();
            
            // ì„¤ì •ì—ì„œ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
            const config = vscode.workspace.getConfiguration('aiCodingAgent.ollama');
            const defaultModel = config.get<string>('defaultModel');
            
            if (defaultModel && this.models.has(defaultModel)) {
                this.activeModelName = defaultModel;
            } else if (this.models.size > 0) {
                // ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
                this.activeModelName = Array.from(this.models.keys())[0];
            }
            
            // ìƒíƒœ í‘œì‹œì¤„ ì—…ë°ì´íŠ¸
            this.updateStatusBar();
        } catch (error) {
            console.error('Ollama ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜:', error);
            vscode.window.showErrorMessage('Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.');
        }
    }
    
    async refreshAvailableModels(): Promise<void> {
        try {
            const response = await axios.get(`${this.endpoint}/api/tags`);
            const modelList = response.data.models || [];
            
            // ê¸°ì¡´ ëª¨ë¸ ëª©ë¡ ì´ˆê¸°í™”
            this.models.clear();
            
            // ìƒˆ ëª¨ë¸ ëª©ë¡ ì¶”ê°€
            for (const model of modelList) {
                const config: OllamaModelConfig = {
                    endpoint: this.endpoint,
                    modelName: model.name,
                    contextSize: this.getContextSizeForModel(model.name)
                };
                
                this.models.set(model.name, new OllamaModel(config));
            }
        } catch (error) {
            console.error('Ollama ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜:', error);
            throw new Error('ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
        }
    }
    
    private getContextSizeForModel(modelName: string): number {
        // ëª¨ë¸ë³„ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ì„¤ì •
        if (modelName.includes('llama3:70b')) return 8192;
        if (modelName.includes('llama3')) return 8192;
        if (modelName.includes('codellama')) return 16384;
        if (modelName.includes('mistral')) return 8192;
        if (modelName.includes('mixtral')) return 32768;
        
        // ê¸°ë³¸ê°’
        return 4096;
    }
    
    getActiveModel(): OllamaModel | null {
        if (!this.activeModelName) return null;
        return this.models.get(this.activeModelName) || null;
    }
    
    async switchModel(modelName: string): Promise<void> {
        if (!this.models.has(modelName)) {
            throw new Error(`ëª¨ë¸ '${modelName}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`);
        }
        
        this.activeModelName = modelName;
        
        // ì„¤ì • ì—…ë°ì´íŠ¸
        const config = vscode.workspace.getConfiguration('aiCodingAgent.ollama');
        await config.update('defaultModel', modelName, vscode.ConfigurationTarget.Global);
        
        // ìƒíƒœ í‘œì‹œì¤„ ì—…ë°ì´íŠ¸
        this.updateStatusBar();
        
        vscode.window.showInformationMessage(`Ollama ëª¨ë¸ì„ '${modelName}'(ìœ¼)ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.`);
    }
    
    getAvailableModels(): string[] {
        return Array.from(this.models.keys());
    }
    
    private updateStatusBar(): void {
        // ìƒíƒœ í‘œì‹œì¤„ ì—…ë°ì´íŠ¸ ë¡œì§ (ìƒíƒœ í‘œì‹œì¤„ êµ¬í˜„ì´ í•„ìš”í•¨)
    }
}
```

### 2.3 ë‹¤ì¤‘ Ollama ì„œë²„ ì§€ì›

ì—¬ëŸ¬ Ollama ì„œë²„ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•œ ì„¤ì • ë° ê´€ë¦¬ ê¸°ëŠ¥:

```typescript
// ai/models/ollamaServerManager.ts
import * as vscode from 'vscode';
import { OllamaManager } from './ollamaManager';

export interface OllamaServerConfig {
    name: string;
    url: string;
    isActive: boolean;
}

export class OllamaServerManager {
    private servers: Map<string, OllamaManager> = new Map();
    private activeServerName: string | null = null;
    
    constructor() {}
    
    async initialize(): Promise<void> {
        // ì„¤ì •ì—ì„œ ì„œë²„ êµ¬ì„± ë¡œë“œ
        const config = vscode.workspace.getConfiguration('aiCodingAgent.ollama');
        const serverConfigs = config.get<OllamaServerConfig[]>('servers') || [];
        
        if (serverConfigs.length === 0) {
            // ê¸°ë³¸ ë¡œì»¬ ì„œë²„ ì¶”ê°€
            serverConfigs.push({
                name: 'Local',
                url: 'http://localhost:11434',
                isActive: true
            });
            
            await config.update('servers', serverConfigs, vscode.ConfigurationTarget.Global);
        }
        
        // ê° ì„œë²„ì— ëŒ€í•œ OllamaManager ìƒì„±
        for (const serverConfig of serverConfigs) {
            const manager = new OllamaManager(serverConfig.url);
            this.servers.set(serverConfig.name, manager);
            
            // í™œì„± ì„œë²„ ì„¤ì •
            if (serverConfig.isActive) {
                this.activeServerName = serverConfig.name;
            }
        }
        
        // í™œì„± ì„œë²„ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì„œë²„ë¥¼ í™œì„±í™”
        if (!this.activeServerName && this.servers.size > 0) {
            this.activeServerName = Array.from(this.servers.keys())[0];
        }
        
        // í™œì„± ì„œë²„ ì´ˆê¸°í™”
        if (this.activeServerName) {
            const activeManager = this.servers.get(this.activeServerName);
            if (activeManager) {
                try {
                    await activeManager.initialize();
                } catch (error) {
                    console.error(`ì„œë²„ '${this.activeServerName}' ì´ˆê¸°í™” ì˜¤ë¥˜:`, error);
                }
            }
        }
    }
    
    getActiveManager(): OllamaManager | null {
        if (!this.activeServerName) return null;
        return this.servers.get(this.activeServerName) || null;
    }
    
    async switchServer(serverName: string): Promise<void> {
        if (!this.servers.has(serverName)) {
            throw new Error(`ì„œë²„ '${serverName}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`);
        }
        
        this.activeServerName = serverName;
        
        // ì„¤ì • ì—…ë°ì´íŠ¸
        const config = vscode.workspace.getConfiguration('aiCodingAgent.ollama');
        const serverConfigs = config.get<OllamaServerConfig[]>('servers') || [];
        
        for (const serverConfig of serverConfigs) {
            serverConfig.isActive = serverConfig.name === serverName;
        }
        
        await config.update('servers', serverConfigs, vscode.ConfigurationTarget.Global);
        
        // ìƒˆë¡œ í™œì„±í™”ëœ ì„œë²„ ì´ˆê¸°í™”
        const activeManager = this.servers.get(this.activeServerName);
        if (activeManager) {
            try {
                await activeManager.initialize();
                vscode.window.showInformationMessage(`Ollama ì„œë²„ë¥¼ '${serverName}'(ìœ¼)ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.`);
            } catch (error) {
                console.error(`ì„œë²„ '${serverName}' ì´ˆê¸°í™” ì˜¤ë¥˜:`, error);
                vscode.window.showErrorMessage(`'${serverName}' ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`);
            }
        }
    }
    
    getAvailableServers(): string[] {
        return Array.from(this.servers.keys());
    }
    
    async addServer(name: string, url: string): Promise<void> {
        if (this.servers.has(name)) {
            throw new Error(`'${name}' ì„œë²„ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.`);
        }
        
        // ìƒˆ ì„œë²„ ì¶”ê°€
        const manager = new OllamaManager(url);
        this.servers.set(name, manager);
        
        // ì„¤ì • ì—…ë°ì´íŠ¸
        const config = vscode.workspace.getConfiguration('aiCodingAgent.ollama');
        const serverConfigs = config.get<OllamaServerConfig[]>('servers') || [];
        
        serverConfigs.push({
            name,
            url,
            isActive: false
        });
        
        await config.update('servers', serverConfigs, vscode.ConfigurationTarget.Global);
        vscode.window.showInformationMessage(`Ollama ì„œë²„ '${name}'ì´(ê°€) ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.`);
    }
    
    async removeServer(name: string): Promise<void> {
        if (!this.servers.has(name)) {
            throw new Error(`ì„œë²„ '${name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`);
        }
        
        // í˜„ì¬ í™œì„± ì„œë²„ì¸ ê²½ìš° ë‹¤ë¥¸ ì„œë²„ë¡œ ì „í™˜
        if (this.activeServerName === name) {
            const availableServers = this.getAvailableServers().filter(s => s !== name);
            if (availableServers.length > 0) {
                await this.switchServer(availableServers[0]);
            } else {
                this.activeServerName = null;
            }
        }
        
        // ì„œë²„ ì œê±°
        this.servers.delete(name);
        
        // ì„¤ì • ì—…ë°ì´íŠ¸
        const config = vscode.workspace.getConfiguration('aiCodingAgent.ollama');
        const serverConfigs = config.get<OllamaServerConfig[]>('servers') || [];
        const updatedConfigs = serverConfigs.filter(s => s.name !== name);
        
        await config.update('servers', updatedConfigs, vscode.ConfigurationTarget.Global);
        vscode.window.showInformationMessage(`Ollama ì„œë²„ '${name}'ì´(ê°€) ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.`);
    }
}
```

## 3. ëª¨ë¸ íŒ©í† ë¦¬ í™•ì¥

ê¸°ì¡´ ModelFactory í´ë˜ìŠ¤ë¥¼ í™•ì¥í•˜ì—¬ Ollama ëª¨ë¸ì„ ì§€ì›:

```typescript
// ai/models/modelFactory.ts
import { AIModel } from './baseModel';
import { OpenAIModel } from './openaiModel';
import { ClaudeModel } from './claudeModel';
import { OllamaModel } from './ollamaModel';
import { OllamaServerManager } from './ollamaServerManager';

export type ModelType = 
    'gpt-4' | 
    'gpt-3.5-turbo' | 
    'claude-3-opus' | 
    'claude-3-sonnet' |
    'ollama';

export class ModelFactory {
    private static ollamaServerManager: OllamaServerManager | null = null;
    
    static async initialize(): Promise<void> {
        // Ollama ì„œë²„ ê´€ë¦¬ì ì´ˆê¸°í™”
        ModelFactory.ollamaServerManager = new OllamaServerManager();
        await ModelFactory.ollamaServerManager.initialize();
    }
    
    static async createModel(type: ModelType, apiKey?: string): Promise<AIModel> {
        switch (type) {
            case 'gpt-4':
                return new OpenAIModel(apiKey || '', 'gpt-4');
            case 'gpt-3.5-turbo':
                return new OpenAIModel(apiKey || '', 'gpt-3.5-turbo');
            case 'claude-3-opus':
                return new ClaudeModel(apiKey || '', 'claude-3-opus-20240229');
            case 'claude-3-sonnet':
                return new ClaudeModel(apiKey || '', 'claude-3-sonnet-20240229');
            case 'ollama':
                if (!ModelFactory.ollamaServerManager) {
                    ModelFactory.ollamaServerManager = new OllamaServerManager();
                    await ModelFactory.ollamaServerManager.initialize();
                }
                
                const activeManager = ModelFactory.ollamaServerManager.getActiveManager();
                if (!activeManager) {
                    throw new Error('í™œì„±í™”ëœ Ollama ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.');
                }
                
                const activeModel = activeManager.getActiveModel();
                if (!activeModel) {
                    throw new Error('í™œì„±í™”ëœ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.');
                }
                
                return activeModel;
            default:
                throw new Error(`ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: ${type}`);
        }
    }
    
    static getOllamaServerManager(): OllamaServerManager | null {
        return ModelFactory.ollamaServerManager;
    }
}
```

## 4. UI êµ¬í˜„

### 4.1 ëª¨ë¸ ì„ íƒ UI

ë‹¤ì–‘í•œ Ollama ëª¨ë¸ì„ ì‰½ê²Œ ì„ íƒí•  ìˆ˜ ìˆëŠ” UI:

```typescript
// ui/ollamaModelSelector.ts
import * as vscode from 'vscode';
import { ModelFactory } from '../ai/models/modelFactory';

export class OllamaModelSelector {
    public static async showModelSelector(): Promise<void> {
        const ollamaManager = ModelFactory.getOllamaServerManager();
        if (!ollamaManager) {
            vscode.window.showErrorMessage('Ollama ì„œë²„ ê´€ë¦¬ìê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
            return;
        }
        
        const activeManager = ollamaManager.getActiveManager();
        if (!activeManager) {
            vscode.window.showErrorMessage('í™œì„±í™”ëœ Ollama ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.');
            return;
        }
        
        // ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë²„ ëª©ë¡
        const servers = ollamaManager.getAvailableServers();
        if (servers.length === 0) {
            vscode.window.showErrorMessage('êµ¬ì„±ëœ Ollama ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.');
            return;
        }
        
        // ì„œë²„ ì„ íƒ (ì—¬ëŸ¬ ì„œë²„ê°€ ìˆëŠ” ê²½ìš°)
        let selectedServer = servers[0];
        if (servers.length > 1) {
            selectedServer = await vscode.window.showQuickPick(servers, {
                placeHolder: 'Ollama ì„œë²„ ì„ íƒ'
            }) || '';
            
            if (!selectedServer) {
                return; // ì‚¬ìš©ìê°€ ì·¨ì†Œí•¨
            }
            
            // ì„ íƒí•œ ì„œë²„ë¡œ ì „í™˜
            if (selectedServer !== ollamaManager.getActiveManager()) {
                await ollamaManager.switchServer(selectedServer);
            }
        }
        
        // ìƒˆë¡œ í™œì„±í™”ëœ ì„œë²„ì˜ OllamaManager ê°€ì ¸ì˜¤ê¸°
        const serverManager = ollamaManager.getActiveManager();
        if (!serverManager) {
            vscode.window.showErrorMessage('Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
            return;
        }
        
        // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
        const models = serverManager.getAvailableModels();
        if (models.length === 0) {
            vscode.window.showErrorMessage('ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.');
            return;
        }
        
        // ëª¨ë¸ ì„ íƒ
        const selectedModel = await vscode.window.showQuickPick(models, {
            placeHolder: 'Ollama ëª¨ë¸ ì„ íƒ'
        });
        
        if (!selectedModel) {
            return; // ì‚¬ìš©ìê°€ ì·¨ì†Œí•¨
        }
        
        // ì„ íƒí•œ ëª¨ë¸ë¡œ ì „í™˜
        await serverManager.switchModel(selectedModel);
        
        // ì „ì—­ ëª¨ë¸ íƒ€ì…ì„ 'ollama'ë¡œ ì„¤ì •
        const config = vscode.workspace.getConfiguration('aiCodingAgent');
        await config.update('modelType', 'ollama', vscode.ConfigurationTarget.Global);
    }
    
    public static async showServerManager(): Promise<void> {
        const ollamaManager = ModelFactory.getOllamaServerManager();
        if (!ollamaManager) {
            vscode.window.showErrorMessage('Ollama ì„œë²„ ê´€ë¦¬ìê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
            return;
        }
        
        // ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë²„ ëª©ë¡ ë° ê´€ë¦¬ ì˜µì…˜
        const servers = ollamaManager.getAvailableServers();
        const options = [
            'â• ìƒˆ ì„œë²„ ì¶”ê°€',
            ...servers.map(s => `ğŸ–¥ï¸ ${s}`)
        ];
        
        const selected = await vscode.window.showQuickPick(options, {
            placeHolder: 'Ollama ì„œë²„ ê´€ë¦¬'
        });
        
        if (!selected) {
            return; // ì‚¬ìš©ìê°€ ì·¨ì†Œí•¨
        }
        
        if (selected === 'â• ìƒˆ ì„œë²„ ì¶”ê°€') {
            // ìƒˆ ì„œë²„ ì¶”ê°€
            const name = await vscode.window.showInputBox({
                prompt: 'ì„œë²„ ì´ë¦„ ì…ë ¥',
                placeHolder: 'ì˜ˆ: ê°œë°œ ì„œë²„'
            });
            
            if (!name) return;
            
            const url = await vscode.window.showInputBox({
                prompt: 'ì„œë²„ URL ì…ë ¥',
                placeHolder: 'ì˜ˆ: http://localhost:11434',
                value: 'http://localhost:11434'
            });
            
            if (!url) return;
            
            try {
                await ollamaManager.addServer(name, url);
            } catch (error) {
                vscode.window.showErrorMessage(`ì„œë²„ ì¶”ê°€ ì‹¤íŒ¨: ${error.message}`);
            }
        } else {
            // ê¸°ì¡´ ì„œë²„ ê´€ë¦¬
            const serverName = selected.substring(2); // 'ğŸ–¥ï¸ ' ì œê±°
            const actions = ['ì „í™˜', 'ì œê±°'];
            
            const action = await vscode.window.showQuickPick(actions, {
                placeHolder: `'${serverName}' ì„œë²„ ì‘ì—… ì„ íƒ`
            });
            
            if (!action) return;
            
            try {
                if (action === 'ì „í™˜') {
                    await ollamaManager.switchServer(serverName);
                } else if (action === 'ì œê±°') {
                    await ollamaManager.removeServer(serverName);
                }
            } catch (error) {
                vscode.window.showErrorMessage(`ì„œë²„ ì‘ì—… ì‹¤íŒ¨: ${error.message}`);
            }
        }
    }
}
```

### 4.2 ëª…ë ¹ ë“±ë¡

Ollama ê´€ë ¨ ëª…ë ¹ ë“±ë¡:

```typescript
// í™•ì¥ ëª…ë ¹ ë“±ë¡ ì‹œ ì¶”ê°€
context.subscriptions.push(
    vscode.commands.registerCommand('aicodingagent.selectOllamaModel', async () => {
        await OllamaModelSelector.showModelSelector();
    })
);

context.subscriptions.push(
    vscode.commands.registerCommand('aicodingagent.manageOllamaServers', async () => {
        await OllamaModelSelector.showServerManager();
    })
);
```

### 4.3 ìƒíƒœ í‘œì‹œì¤„ ì•„ì´í…œ

í˜„ì¬ ì„ íƒëœ Ollama ëª¨ë¸ì„ í‘œì‹œí•˜ëŠ” ìƒíƒœ í‘œì‹œì¤„ ì•„ì´í…œ:

```typescript
// ui/statusBar.ts
import * as vscode from 'vscode';
import { ModelFactory } from '../ai/models/modelFactory';

export class AIStatusBarItem {
    private static instance: AIStatusBarItem;
    private statusBarItem: vscode.StatusBarItem;
    
    private constructor() {
        this.statusBarItem = vscode.window.createStatusBarItem(
            vscode.StatusBarAlignment.Right,
            100
        );
        this.statusBarItem.command = 'aicodingagent.selectModel';
        this.update();
        this.statusBarItem.show();
    }
    
    public static getInstance(): AIStatusBarItem {
        if (!AIStatusBarItem.instance) {
            AIStatusBarItem.instance = new AIStatusBarItem();
        }
        return AIStatusBarItem.instance;
    }
    
    public async update(): Promise<void> {
        const config = vscode.workspace.getConfiguration('aiCodingAgent');
        const modelType = config.get<string>('modelType') || 'gpt-4';
        
        let modelName = modelType;
        
        if (modelType === 'ollama') {
            const ollamaManager = ModelFactory.getOllamaServerManager();
            if (ollamaManager) {
                const activeManager = ollamaManager.getActiveManager();
                if (activeManager) {
                    const activeModel = activeManager.getActiveModel();
                    if (activeModel) {
                        // Ollama ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: llama3:8b -> Llama3 8B)
                        const rawName = activeModel.modelName;
                        const parts = rawName.split(':');
                        
                        if (parts.length > 1) {
                            const baseName = parts[0].charAt(0).toUpperCase() + parts[0].slice(1);
                            modelName = `${baseName} ${parts[1].toUpperCase()}`;
                        } else {
                            modelName = rawName.charAt(0).toUpperCase() + rawName.slice(1);
                        }
                    }
                }
            }
        }
        
        this.statusBarItem.text = `$(robot) ${modelName}`;
        this.statusBarItem.tooltip = `í˜„ì¬ AI ëª¨ë¸: ${modelName}\ní´ë¦­í•˜ì—¬ ëª¨ë¸ ë³€ê²½`;
    }
    
    public dispose(): void {
        this.statusBarItem.dispose();
    }
}
```

## 5. ì„¤ì • ìŠ¤í‚¤ë§ˆ

Ollama ê´€ë ¨ ì„¤ì •ì„ ìœ„í•œ package.json ì„¤ì • ì¶”ê°€:

```json
"contributes": {
    "configuration": {
        "title": "AI ì½”ë”© ì—ì´ì „íŠ¸",
        "properties": {
            "aiCodingAgent.modelType": {
                "type": "string",
                "enum": ["gpt-4", "gpt-3.5-turbo", "claude-3-opus", "claude-3-sonnet", "ollama"],
                "default": "gpt-4",
                "description": "ì‚¬ìš©í•  AI ëª¨ë¸ íƒ€ì…"
            },
            "aiCodingAgent.ollama.defaultModel": {
                "type": "string",
                "default": "",
                "description": "ê¸°ë³¸ Ollama ëª¨ë¸"
            },
            "aiCodingAgent.ollama.servers": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "name": {
                            "type": "string",
                            "description": "Ollama ì„œë²„ ì´ë¦„"
                        },
                        "url": {
                            "type": "string",
                            "description": "Ollama ì„œë²„ URL"
                        },
                        "isActive": {
                            "type": "boolean",
                            "description": "ì´ ì„œë²„ê°€ í˜„ì¬ í™œì„± ì„œë²„ì¸ì§€ ì—¬ë¶€"
                        }
                    }
                },
                "default": [
                    {
                        "name": "Local",
                        "url": "http://localhost:11434",
                        "isActive": true
                    }
                ],
                "description": "Ollama ì„œë²„ ëª©ë¡"
            }
        }
    },
    "commands": [
        {
            "command": "aicodingagent.selectOllamaModel",
            "title": "AI ì½”ë”© ì—ì´ì „íŠ¸: Ollama ëª¨ë¸ ì„ íƒ"
        },
        {
            "command": "aicodingagent.manageOllamaServers",
            "title": "AI ì½”ë”© ì—ì´ì „íŠ¸: Ollama ì„œë²„ ê´€ë¦¬"
        }
    ]
}
```

## 6. í•µì‹¬ ì´ì 

### 6.1 í”„ë¼ì´ë²„ì‹œ ë° ë³´ì•ˆ ê°•í™”

- ë¯¼ê°í•œ ì½”ë“œê°€ ì™¸ë¶€ APIë¡œ ì „ì†¡ë˜ì§€ ì•ŠìŒ
- ì¸í„°ë„· ì—°ê²° ì—†ì´ë„ AI ì½”ë”© ì§€ì› ê°€ëŠ¥
- íšŒì‚¬ ë‚´ë¶€ë§ì—ì„œë„ ì•ˆì „í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥

### 6.2 ë‹¤ì–‘í•œ ëª¨ë¸ ì‹¤í—˜

- íŠ¹ì • ì‘ì—…ì— ìµœì í™”ëœ ë‹¤ì–‘í•œ ëª¨ë¸ í™œìš© ê°€ëŠ¥
  - CodeLlama: ì½”ë“œ ìƒì„± ë° ì™„ì„±ì— ìµœì í™”
  - Llama 3: ì¼ë°˜ì ì¸ ì§ˆì˜ì‘ë‹µ ë° ì„¤ëª…
  - Mistral/Mixtral: ê²½ëŸ‰ ëª¨ë¸ê³¼ MoE ëª¨ë¸ ì§€ì›

### 6.3 ë¹„ìš© íš¨ìœ¨ì„±

- API ì‚¬ìš©ë£Œ ì—†ì´ ë¬´ì œí•œ ì‚¬ìš© ê°€ëŠ¥
- í•˜ë“œì›¨ì–´ ìš©ëŸ‰ ë‚´ì—ì„œ ë‹¤ì–‘í•œ ëª¨ë¸ í™œìš©

### 6.4 ì‚¬ìš©ì ì •ì˜ ë° í™•ì¥ì„±

- íŠ¹ì • ë„ë©”ì¸ì´ë‚˜ íšŒì‚¬ ì½”ë“œë² ì´ìŠ¤ì— fine-tuningëœ ëª¨ë¸ í™œìš© ê°€ëŠ¥
- ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì§€ì›

## 7. ë„ì „ ê³¼ì œ ë° í•´ê²°ì±…

### 7.1 ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­

- **ë„ì „ ê³¼ì œ**: ë¡œì»¬ LLMì€ ìƒë‹¹í•œ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ í•„ìš”
- **í•´ê²°ì±…**: ë‹¤ì–‘í•œ í¬ê¸°ì˜ ëª¨ë¸ ì§€ì› (7Bë¶€í„° 70Bê¹Œì§€)ê³¼ quantized ëª¨ë¸ ì§€ì›

### 7.2 ëª¨ë¸ í’ˆì§ˆ

- **ë„ì „ ê³¼ì œ**: ì¼ë¶€ ë¡œì»¬ ëª¨ë¸ì€ ìƒìš© API ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ
- **í•´ê²°ì±…**: íŠ¹í™”ëœ ì½”ë”© ëª¨ë¸(CodeLlama ë“±) ì‚¬ìš© ë° í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• ì§€ì›

### 7.3 ì„¤ì¹˜ ë³µì¡ì„±

- **ë„ì „ ê³¼ì œ**: ì‚¬ìš©ìê°€ Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•„ìš”
- **í•´ê²°ì±…**: ìë™í™”ëœ ì„¤ì¹˜ ê°€ì´ë“œ ë° ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì§€ì›

## 8. í–¥í›„ ê°œì„  ë°©í–¥

1. **Ollama ëª¨ë¸ ìë™ ì œì•ˆ**: ì‘ì—… ìœ í˜•ì— ë”°ë¼ ìµœì ì˜ ëª¨ë¸ ìë™ ì„ íƒ
2. **ëª¨ë¸ íŠœë‹ ì¸í„°í˜ì´ìŠ¤**: VS Codeì—ì„œ ì§ì ‘ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°ì • ê°€ëŠ¥
3. **ì»¤ìŠ¤í…€ ëª¨ë¸ í•™ìŠµ**: í”„ë¡œì íŠ¸ ì½”ë“œë² ì´ìŠ¤ë¡œ ëª¨ë¸ fine-tuning ì§€ì›
4. **ë¶„ì‚° ì¶”ë¡ **: ì—¬ëŸ¬ Ollama ì¸ìŠ¤í„´ìŠ¤ ê°„ ë¶€í•˜ ë¶„ì‚° ì§€ì› 